###info
title=Scala IDE + Spark 연동 예제(for Windows7)
created=2015-07-14
ad=true
category=Spark
tags=Spark
###
#p Apache Spark 라이브러리를 로컬 머신에서 한번 돌려보자는 생각에 포스팅을 쓰게 되었습니다. Windows7에서 구동한 포스팅입니다.


#h2 Preparation
#p 이번 포스팅에서는 {$/blog/82|스칼라 IDE}를 사용해서 진행합니다. 따라서 스칼라 IDE가 별도로 없다면 링크를 클릭해서 진행하신 뒤에 포스팅을 이어서 보시기 바랍니다. :D

#h2 Import Spark Library
#p 스파크 라이브러리를 받는 방법은 두가지가 있습니다. 하나는 스파크 홈페이지에서 직접 내려받은 라이브러리를 사용하는 것과 메이븐을 이용하는것입니다.

#p 두 방법 모두 다음과 같은 작업은 공통으로 진행합니다. 스칼라 IDE에서 다음과 같은 순서로 프로젝트를 생성 합니다.

###box.yellow
#p File > Scala Project > Project Name : spark, JRE : JavaSE-1.8 > Finish
###

#h3 Using Maven
#p 메이븐을 이용하기 위해 스칼라 프로젝트를 메이븐 프로젝트로 변환시켜 줍니다.


###box.yellow
#p 프로젝트 탐색기에서 해당 프로젝트 오른쪽 클릭 > Configure > Convert To Maven Project
###

#p 그 후 생성된 `pom.xml` 파일에 다음을 추가해줍니다.

###code.xml
&lt;dependencies>
	&lt;dependency>
		&lt;groupId>org.apache.hadoop&lt;/groupId>
		&lt;artifactId>hadoop-client&lt;/artifactId>
		&lt;version>1.2.1&lt;/version>
	&lt;/dependency>

	&lt;dependency>
		&lt;groupId>org.apache.spark&lt;/groupId>
		&lt;artifactId>spark-core_2.10&lt;/artifactId>
		&lt;version>1.4.1&lt;/version>
	&lt;/dependency>
&lt;/dependencies>
###

#p 버전은 각자 알맞게 설정하시면 됩니다.


#h3 Using Direct Library
#p 이번 방법에서 진행하는 라이브러리는 스파크를 다운로드 받아야 합니다. 해당 내용은 {$/blog/164|Apache Spark Overview & Example} 포스트를 참조하세요.


#h4 Add Jar File
#p 다음과 같은 목록의 jar 파일을 build path에 추가합니다.

###tab
###ul 
#li datanucleus-api-jdo-3.2.6.jar
#li datanucleus-core-3.2.10.jar
#li datanucleus-rdbms-3.2.9.jar
#li spark-1.4.0-yarn-shuffle.jar
#li spark-assembly-1.4.0-hadoop2.6.0.jar
#li spark-examples-1.4.0-hadoop2.6.0.jar
###
###

#p 해당 압축 파일을 다운로드 받으려면 {$https://spark.apache.org/downloads.html|Spark 다운로드}를 클릭하세요. 여기서 `spark-{version}.tgz` 파일을 내려 받으면 됩니다.

#p 위의 jar 파일은 `spark-1.4.0-bin-hadoop2.6.tgz` 파일을 내려 받았다면 최하단 디렉토리 중 lib 디렉토리에 있습니다.

#p build path에 jar 파일을 추가한 이후 스칼라 컴파일러 버전을 맞춰줍니다. 저는 2.10.x 버전대로 맞췄습니다.


#h4 Run Configuration
#p 해당 프로젝트의 Run Configration을 설정해야 합니다. `Run > Run Configuration`을 클릭해서 설정 윈도우를 띄웁니다.

#p 그리고 Environment 탭에서 Name을 `HADOOP_HOME` 그리고 Value를 `/YourPath/hadoop-common-2.2.0-bin-master`라고 맞춰줍니다.

#h4 hadoop-common-2.2.0-bin-master
#p 윗절에서 지정한 HADOOP_HOME의 설정을 위해 추가로 다운로드 받을 것이 있습니다. {$https://github.com/srccodes/hadoop-common-2.2.0-bin|https://github.com/srccodes/hadoop-common-2.2.0-bin}를 클릭해서 Zip 데이터를 다운로드 받습니다. 그리고 이것을 아까 지정했던 HADOOP_HOME 경로에 맞춰서 압축을 해제해 줍니다.

#h2 Start Spark Context
#p 스파크 관련 라이브러리를 import 했다면 이번엔 간단하게 테스트를 해봅시다. 다음과 같은 스칼라 코드를 작성합니다.

###code.scala
import org.apache.spark.SparkContext
import org.apache.spark.SparkConf

object Spark {
 def main(args: Array[String]): Unit = {
   val testFile = "/YOUR_TEST_FILE_PATH/test.txt"
   val conf = new SparkConf().setAppName("My First Spark App").setMaster("local[2]")
   var sc = new SparkContext(conf)
   val testData = sc.textFile(testFile, 2).cache()

   println(testData.count())
 }
}
###

#p 저는 결과 값으로 {!8}이 나오긴 했습니다. 이제 로컬에서 테스트가 가능하겠네요. :)


#h3 Using Remote Hadoop Server
#p 만약, 하둡 서버를 별도로 돌리고 있다면 아래처럼 `hdfs://`를 붙여서 직접 파일에 접근할 수도 있습니다.

###code.scala
val testFile = "hdfs://YOUR_HADOOP_NODE_IP_OR_NAME:9000/..."
###

#p 이 경우에 스파크에 포함된 Hadoop 라이브러리와 Hadoop이 설치된 서버의 버전을 꼭 맞춰서 테스트 하시기 바랍니다. :)

